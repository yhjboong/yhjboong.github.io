---
title: "Benchmark Harmonization and Model Similarity Analysis"
excerpt: "Harmonizing major AI evaluation benchmarks (LiveBench, HELM, LMSYS Arena) and developing model similarity maps at FORESEER Lab, University of Michigan."
collection: portfolio
share: false
---

## Project Overview

At the FORESEER Lab (University of Michigan), under the supervision of [Prof. Qiaozhu Mei](https://www.si.umich.edu/people/qiaozhu-mei), I'm working on a critical challenge in AI evaluation: harmonizing diverse benchmarks to enable principled cross-benchmark comparison and understanding model relationships across different evaluation frameworks.

## Key Contributions

- **Unified three major benchmarks**: LiveBench, HELM, and LMSYS Arena into a coherent evaluation framework
- **Standardized model identifiers** across different evaluation frameworks for consistent comparison
- **Deduplicated variants** and retained latest versions per modelâ€“objective pair for clean analysis
- **Enabled principled cross-benchmark comparison** for the first time at this scale
- **Constructed comprehensive similarity maps** using multidimensional scaling (MDS) techniques
- **Implemented centrality analysis** to identify key models and relationships in the evaluation space
- **Reported rigorous fit metrics** including raw stress and Stress-1 measurements for validation
- **Verified stability** under injected noise and sub-sampling conditions for robustness
- **Diagnosed sources of misalignment** between different evaluation approaches
- **Proposed compact task subsets** that preserve similarity structure while reducing computational cost

## Technical Skills Demonstrated

- **Cross-platform data integration** and model ID standardization across diverse systems
- **Variant deduplication algorithms** for large-scale dataset cleaning and processing
- **Multi-benchmark data alignment** and validation techniques for complex datasets
- **Multidimensional scaling implementation** and optimization for high-dimensional analysis
- **Statistical stress metric calculation** and interpretation for dimensionality reduction
- **Noise injection protocols** and stability testing for robust experimental validation
- **Interactive data visualization** development for model similarity and relationships
- **Meta-analysis methodology** for evaluation framework comparison and optimization

## Impact and Applications

This research addresses fundamental challenges in AI evaluation and provides practical benefits:
- **Reduces evaluation costs** while maintaining assessment quality through efficient subset selection
- **Enables meta-analysis** across previously incompatible benchmarks for broader insights
- **Provides insights** into model relationships and evaluation redundancies across the field
- **Guides future benchmark development** through systematic analysis of existing frameworks
- **Supports model selection** and comparison through comprehensive similarity analysis
- **Improves evaluation standardization** across the AI research community

## Research Significance

This work contributes to making AI evaluation more efficient, standardized, and scientifically rigorous across the field. The methodological innovations in cross-benchmark standardization and robust statistical validation of similarity structures provide a foundation for future evaluation research and practical frameworks for evaluation cost reduction in both academic and industrial settings.