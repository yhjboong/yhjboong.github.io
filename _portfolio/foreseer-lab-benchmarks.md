---
title: "Benchmark Harmonization and Model Similarity Analysis"
excerpt: "Harmonizing major AI evaluation benchmarks (LiveBench, HELM, LMSYS Arena) and developing model similarity maps at FORESEER Lab, University of Michigan."
collection: portfolio
share: false
---

## Project Overview

At the FORESEER Lab (University of Michigan), under the supervision of [Prof. Qiaozhu Mei](https://www.si.umich.edu/people/qiaozhu-mei), I'm working on a critical challenge in AI evaluation: harmonizing diverse benchmarks to enable principled cross-benchmark comparison and understanding model relationships.

## Key Achievements

### Benchmark Harmonization
- **Unified three major benchmarks**: LiveBench, HELM, and LMSYS Arena
- **Standardized model identifiers** across different evaluation frameworks
- **Deduplicated variants** and retained latest versions per modelâ€“objective pair
- **Enabled principled cross-benchmark comparison** for the first time at this scale

### Model Similarity Analysis
- **Constructed comprehensive similarity maps** using multidimensional scaling (MDS)
- **Implemented centrality analysis** to identify key models and relationships
- **Reported rigorous fit metrics** including raw stress and Stress-1 measurements
- **Verified stability** under injected noise and sub-sampling conditions

### Evaluation Optimization
- **Diagnosed sources of misalignment** between different evaluation approaches
- **Proposed compact task subsets** that preserve similarity structure while reducing computational cost
- **Developed methodologies** for efficient benchmark subset selection

## Technical Contributions

### Data Processing
- Cross-platform model ID standardization
- Variant deduplication algorithms
- Multi-benchmark data alignment and validation

### Statistical Analysis
- Multidimensional scaling implementation and optimization
- Stress metric calculation and interpretation
- Noise injection and stability testing protocols

### Visualization and Interpretation
- Interactive model similarity visualizations
- Centrality-based model importance rankings
- Benchmark alignment diagnostic tools

## Research Impact

This work addresses fundamental challenges in AI evaluation:
- **Reduces evaluation costs** while maintaining assessment quality
- **Enables meta-analysis** across previously incompatible benchmarks  
- **Provides insights** into model relationships and evaluation redundancies
- **Guides future benchmark development** through systematic analysis

## Methodological Innovations
- Novel approaches to cross-benchmark standardization
- Robust statistical validation of similarity structures
- Practical frameworks for evaluation cost reduction

This research contributes to making AI evaluation more efficient, standardized, and scientifically rigorous across the field.
